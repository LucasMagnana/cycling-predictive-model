{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import copy\n",
    "from sklearn.cluster import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import geopy.distance\n",
    "from scipy.spatial.distance import *\n",
    "import random\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import KDTree\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "from folium.plugins import HeatMap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import python.data as data\n",
    "import python.display as dp\n",
    "import python.voxels as voxel\n",
    "import python.metric as metric\n",
    "import python.clustering as cl\n",
    "import python.RNN as RNN\n",
    "#import python.learning as learning\n",
    "#from python.NN import *\n",
    "\n",
    "project_folder = \"veleval\"\n",
    "\n",
    "display = False\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "graph_size = (13,6)\n",
    "\n",
    "new_distance_matrix = False\n",
    "\n",
    "save_dbscan_oservations = False\n",
    "\n",
    "new_voxels_clustering = True\n",
    "save_voxels_clustering = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lyon = (45.74846, 4.84671)\\nst_etienne = (45.4333, 4.4)\\nG_lyon = ox.graph_from_point(lyon, dist=10000)\\nG_stetienne = ox.graph_from_point(st_etienne, dist=7500)\\nG = G_lyon\\nwith open(\"files/\"+project_folder+\"/city_graphs/city.ox\", \"wb\") as outfile:\\n    pickle.dump(G_lyon, outfile)\\nwith open(\"files/\"+project_folder+\"/city_graphs/city_2.ox\", \"wb\") as outfile:\\n    pickle.dump(G_stetienne, outfile)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''lyon = (45.74846, 4.84671)\n",
    "st_etienne = (45.4333, 4.4)\n",
    "G_lyon = ox.graph_from_point(lyon, dist=10000)\n",
    "G_stetienne = ox.graph_from_point(st_etienne, dist=7500)\n",
    "G = G_lyon\n",
    "with open(\"files/\"+project_folder+\"/city_graphs/city.ox\", \"wb\") as outfile:\n",
    "    pickle.dump(G_lyon, outfile)\n",
    "with open(\"files/\"+project_folder+\"/city_graphs/city_2.ox\", \"wb\") as outfile:\n",
    "    pickle.dump(G_stetienne, outfile)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.mapmatching(\"files/\"+project_folder+\"/data_processed/observations.df\", \n",
    "                 \"files/\"+project_folder+\"/data_processed/observations_matched.df\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.simplify_gps(\"files/\"+project_folder+\"/data_processed/observations_matched.df\", \n",
    "                  \"files/\"+project_folder+\"/data_processed/observations_matched_simplified.df\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.pathfinding_osmnx(\"files/\"+project_folder+\"/data_processed/observations_matched_simplified.df\", \n",
    "                       \"files/\"+project_folder+\"/data_processed/osmnx_pathfinding.df\", \n",
    "                       \"files/\"+project_folder+\"/city_graphs\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.simplify_gps(\"files/\"+project_folder+\"/data_processed/osmnx_pathfinding.df\", \n",
    "                  \"files/\"+project_folder+\"/data_processed/osmnx_pathfinding_simplified.df\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.compute_distance(\"files/\"+project_folder+\"/data_processed/observations_matched_simplified.df\",\n",
    "                      \"files/\"+project_folder+\"/distances/distances_observations.tab\")\n",
    "data.compute_distance(\"files/\"+project_folder+\"/data_processed/osmnx_pathfinding_simplified.df\", \n",
    "                      \"files/\"+project_folder+\"/distances/distances_osmnx.tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bikepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.bikepath_fusion(\"files/\"+project_folder+\"/data_processed/osm_bikepath.df\", \n",
    "#                     \"files/\"+project_folder+\"/data_processed/bikepath_fusioned.df\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"files/\"+project_folder+\"/data_processed/observations_matched_simplified.df\",'rb') as infile:\n",
    "    df_simplified = pickle.load(infile)\n",
    "tab_routes_voxels_simplified, tab_routes_voxels_simplified_global, dict_voxels_simplified = voxel.generate_voxels(\n",
    "    df_simplified, df_simplified.iloc[0][\"route_num\"], df_simplified.iloc[-1][\"route_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/\"+project_folder+\"/data_processed/osmnx_pathfinding_simplified.df\",'rb') as infile:\n",
    "    df_pathfinding = pickle.load(infile)\n",
    "tab_routes_voxels_pathfinding, tab_routes_voxels_pathfinding_global, dict_voxels_pathfinding = voxel.generate_voxels(\n",
    "    df_pathfinding, df_pathfinding.iloc[0][\"route_num\"], df_pathfinding.iloc[-1][\"route_num\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899\n"
     ]
    }
   ],
   "source": [
    "print(len(tab_routes_voxels_simplified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with open(\"files/\"+project_folder+\"/data_processed/bikepath_fusioned.df\", \"rb\") as outfile:\\n    df_bikepath = pickle.load(outfile)\\n    df_bikepath = pd.DataFrame(df_bikepath, dtype=object)\\n\\n_, _, dict_voxels_bikepath = voxel.generate_voxels(df_bikepath, df_bikepath.iloc[0][\"route_num\"], df_bikepath.iloc[-1][\"route_num\"], True)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with open(\"files/\"+project_folder+\"/data_processed/bikepath_fusioned.df\", \"rb\") as outfile:\n",
    "    df_bikepath = pickle.load(outfile)\n",
    "    df_bikepath = pd.DataFrame(df_bikepath, dtype=object)\n",
    "\n",
    "_, _, dict_voxels_bikepath = voxel.generate_voxels(df_bikepath, df_bikepath.iloc[0][\"route_num\"], df_bikepath.iloc[-1][\"route_num\"], True)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "map=None\n",
    "if(display):\n",
    "    num_vox = -1\n",
    "    tab_vox = []\n",
    "    num_route = 0\n",
    "    df_display = df_simplified[df_simplified[\"route_num\"]==num_route]\n",
    "    df_display[\"type\"] = 0\n",
    "    for v in tab_routes_voxels_simplified[num_route]:\n",
    "        vox_str = v.split(\";\")\n",
    "        vox_int = [int(vox_str[0]), int(vox_str[1])]\n",
    "        tab_vox += voxel.get_voxel_points(vox_int, num_vox)\n",
    "        num_vox -= 1\n",
    "\n",
    "    df = pd.DataFrame(tab_vox, columns=[\"lat\", \"lon\", \"route_num\", \"type\"])\n",
    "    df_display = df_display.append(df)\n",
    "    map = dp.display(df_display, color=\"type\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):\n",
    "    with open(\"files/\"+project_folder+\"/data_processed/observations_matched_simplified.df\",'rb') as infile:\n",
    "            df_simplified = pickle.load(infile)\n",
    "\n",
    "    nb_routes = 1\n",
    "    min_routes = 2\n",
    "    df_simplified[\"type\"] = 0\n",
    "    df_display = df_simplified[(df_simplified[\"route_num\"]<=nb_routes)]\n",
    "    #print(df_display)\n",
    "    _, _, dict_voxels = voxel.generate_voxels(df_display, 0, nb_routes)\n",
    "    tab_vox = voxel.get_voxels_with_min_routes(dict_voxels, min_routes)\n",
    "    print(len(tab_vox)/4)\n",
    "\n",
    "    df = pd.DataFrame(tab_vox, columns=[\"lat\", \"lon\", \"route_num\", \"type\"])\n",
    "    df_display = df_display.append(df)\n",
    "    map = dp.display(df_display, color=\"type\") \n",
    "    #print(tab_vox)\n",
    "    #print(df)\n",
    "    \n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):     \n",
    "    tab = []\n",
    "    for key in dict_voxels_simplified:\n",
    "        vox_str = key.split(\";\")\n",
    "        vox_int = [int(vox_str[0]), int(vox_str[1])]\n",
    "        vox_pos = voxel.get_voxel_points(vox_int, 0)\n",
    "        tab.append([vox_pos[0][0], vox_pos[0][1], dict_voxels_simplified[key][\"cyclability_coeff\"]])\n",
    "\n",
    "    df = pd.DataFrame(tab, columns=[\"lat\", \"lon\", \"Cyclability coefficient\"])\n",
    "    \n",
    "    map = folium.Map(location=[df.iloc[0][\"lat\"],df.iloc[0][\"lon\"]], zoom_start=11)\n",
    "    HeatMap(data=df.values.tolist(), max_zoom=13, radius=9, blur = 1, min_opacity = 0, max_val = 1).add_to(map)\n",
    "\n",
    "    \n",
    "    '''fig = px.scatter_mapbox(df, lat=\"lat\", lon=\"lon\",  color=\"Cyclability coefficient\", size=\"Cyclability coefficient\", zoom=11)\n",
    "    fig.show()\n",
    "    fig.write_image(\"images/heatmap_simplified.png\")'''\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):\n",
    "    tab = []\n",
    "    for key in dict_voxels_pathfinding:\n",
    "        vox_str = key.split(\";\")\n",
    "        vox_int = [int(vox_str[0]), int(vox_str[1])]\n",
    "        vox_pos = voxel.get_voxel_points(vox_int, 0)\n",
    "        tab.append([vox_pos[0][0], vox_pos[0][1], dict_voxels_pathfinding[key][\"cyclability_coeff\"]])\n",
    "\n",
    "    df = pd.DataFrame(tab, columns=[\"lat\", \"lon\", \"Cyclability coefficient\"])\n",
    "    \n",
    "    map = folium.Map(location=[df.iloc[0][\"lat\"],df.iloc[0][\"lon\"]], control_scale=True, zoom_start=11)\n",
    "    HeatMap(data=df[['lat', 'lon', 'Cyclability coefficient']].values.tolist(), max_zoom=13, radius=9, blur = 1, min_opacity = 0, max_val = 1).add_to(map)\n",
    "    \n",
    "    '''fig = px.scatter_mapbox(df, lat=\"lat\", lon=\"lon\",  color=\"Cyclability coefficient\", size=\"Cyclability coefficient\", zoom=11)\n",
    "    fig.show()\n",
    "    fig.write_image(\"images/heatmap_mapbox.png\")'''\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):\n",
    "    tab_routes_voxels_common = []\n",
    "    dict_voxels_common = {}\n",
    "    tab = []\n",
    "\n",
    "    for i in range(len(tab_routes_voxels_pathfinding_global)):\n",
    "        tab_routes_voxels_common.append(list(set(tab_routes_voxels_pathfinding_global[i]).intersection(set(tab_routes_voxels_simplified_global[i]))))\n",
    "\n",
    "    for i in range(len(tab_routes_voxels_common)):\n",
    "        for key in tab_routes_voxels_common[i]:\n",
    "            if key not in dict_voxels_common:\n",
    "                dict_voxels_common[key] = [i]\n",
    "            else:\n",
    "                dict_voxels_common[key].append(i)\n",
    "\n",
    "    for key in dict_voxels_common:\n",
    "        tab_routes = dict_voxels_common[key]\n",
    "        vox_str = key.split(\";\")\n",
    "        vox_int = [int(vox_str[0]), int(vox_str[1])]\n",
    "        vox_pos = voxel.get_voxel_points(vox_int, 0)\n",
    "        if(len(tab_routes) >= 0):\n",
    "            tab.append([vox_pos[0][0], vox_pos[0][1], len(tab_routes)])\n",
    "\n",
    "    df = pd.DataFrame(tab, columns=[\"lat\", \"lon\", \"value\"])\n",
    "    \n",
    "    map = folium.Map(location=[df.iloc[0][\"lat\"],df.iloc[0][\"lon\"]], control_scale=True, zoom_start=11)\n",
    "    HeatMap(data=df.values.tolist(), max_zoom=13, radius=9, blur = 1, min_opacity = 0, max_val = 1).add_to(map)\n",
    "    \n",
    "    '''fig = px.scatter_mapbox(df, lat=\"lat\", lon=\"lon\",  color=\"value\", size=\"value\", zoom=10)\n",
    "    fig.show()'''\n",
    "    \n",
    "#map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):\n",
    "    max_value = 0\n",
    "    tab = []\n",
    "    for key in dict_voxels_pathfinding:\n",
    "        tab_routes = dict_voxels_pathfinding[key][\"tab_routes_starting\"]\n",
    "        vox_str = key.split(\";\")\n",
    "        vox_int = [int(vox_str[0]), int(vox_str[1])]\n",
    "        vox_pos =voxel. get_voxel_points(vox_int, 0)\n",
    "        if(len(tab_routes) >= 0):\n",
    "            tab.append([vox_pos[0][0], vox_pos[0][1], len(tab_routes)])\n",
    "            if(len(tab_routes)>max_value):\n",
    "                max_value+=1\n",
    "\n",
    "    df = pd.DataFrame(tab, columns=[\"lat\", \"lon\", \"value\"])\n",
    "    \n",
    "    map = folium.Map(location=[df.iloc[0][\"lat\"],df.iloc[0][\"lon\"]], control_scale=True, zoom_start=11)\n",
    "    HeatMap(data=df.values.tolist(), max_zoom=13, radius=9, blur = 1, min_opacity = 0.2, max_val = max_value).add_to(map)\n",
    "    \n",
    "    '''fig = px.scatter_mapbox(df, lat=\"lat\", lon=\"lon\",  color=\"value\", size=\"value\", zoom=10)\n",
    "    fig.show()'''\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphes distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):\n",
    "    with open(\"files/\"+project_folder+\"/distances/distances_observations.tab\",'rb') as infile:\n",
    "        tab_distances = pickle.load(infile)    \n",
    "\n",
    "    with open(\"files/\"+project_folder+\"/distances/distances_osmnx.tab\",'rb') as infile:\n",
    "        tab_distances_pf = pickle.load(infile)   \n",
    "\n",
    "    print(len(tab_distances_pf))\n",
    "    \n",
    "    fig = plt.figure(figsize=graph_size)\n",
    "    ax = plt.axes()\n",
    "    x = np.linspace(0, len(tab_distances_pf), len(tab_distances_pf))\n",
    "    plt.plot(x, tab_distances, color='red', linewidth=3.5, label='Distance réel')\n",
    "    plt.plot(x, tab_distances_pf, color='blue', linewidth=3.5, label='Distance plus court chemin')\n",
    "    \n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):\n",
    "\n",
    "    tab_distances_relatives = []\n",
    "    for i in range(len(tab_distances)):\n",
    "        tab_distances_relatives.append(tab_distances[i]-tab_distances_pf[i])\n",
    "        \n",
    "    fig = plt.figure(figsize=graph_size)\n",
    "    ax = plt.axes()\n",
    "    x = np.linspace(0, len(tab_distances_relatives), len(tab_distances_relatives))\n",
    "    plt.plot(x, tab_distances_relatives, color='blue', linewidth=3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):\n",
    "    tab_distances_relatives.sort()\n",
    "    tab_display = []\n",
    "    tab_axis = []\n",
    "    dist_max = 0\n",
    "    nb_routes = 0\n",
    "    for dist in tab_distances_relatives:\n",
    "        if(dist > dist_max):\n",
    "            tab_display.append(nb_routes/len(tab_distances_relatives))\n",
    "            tab_axis.append(dist_max)\n",
    "            dist_max += 0.2\n",
    "        nb_routes += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    fig = plt.figure(figsize=graph_size)\n",
    "    ax = plt.axes()\n",
    "    plt.plot(tab_axis, tab_display, color='blue', linewidth=3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrice de distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of non-zero in diag :  0\n"
     ]
    }
   ],
   "source": [
    "if(new_distance_matrix):\n",
    "    distance_matrix = np.empty([df_simplified.iloc[-1][\"route_num\"]+1, df_simplified.iloc[-1][\"route_num\"]+1])\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "\n",
    "    for i in range(df_simplified.iloc[-1][\"route_num\"]+1):\n",
    "\n",
    "        for j in range(i, df_simplified.iloc[-1][\"route_num\"]+1):\n",
    "            coeff = metric.get_distance_voxels(i, j, tab_routes_voxels_simplified_global)\n",
    "            #coeff = metric.get_distance_euclidian(df_cluster[df_cluster[\"route_num\"]==i+1], df_cluster[df_cluster[\"route_num\"]==j+1], pca)\n",
    "            #coeff = metric.get_distance_hausdorff(df_cluster[df_cluster[\"route_num\"]==i+1], df_cluster[df_cluster[\"route_num\"]==j+1])\n",
    "            distance_matrix[i][j] = coeff[0]\n",
    "            distance_matrix[j][i] = coeff[1]\n",
    "    \n",
    "    data.check_file(\"files/\"+project_folder+\"/clustering/distances_dbscan.mx\", [])\n",
    "    with open(\"files/\"+project_folder+\"/clustering/distances_dbscan.mx\",'wb') as outfile:\n",
    "        pickle.dump(distance_matrix, outfile)\n",
    "    \n",
    "else:\n",
    "    with open(\"files/\"+project_folder+\"/clustering/distances_dbscan.mx\",'rb') as infile:\n",
    "        distance_matrix = pickle.load(infile)\n",
    "\n",
    "print(\"number of non-zero in diag : \", len(np.nonzero(np.diagonal(distance_matrix))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphes clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_variable = 0.1\n",
    "stop_variable = 0.9\n",
    "step_variable = 0.1\n",
    "\n",
    "tab_nb_clusters = []\n",
    "tab_noise = []\n",
    "tab_nb_mini_clusters = []\n",
    "tab_nb_big_clusters = []\n",
    "tab_mean = []\n",
    "tab_silhouette = []\n",
    "\n",
    "for variable in np.arange(start_variable, stop_variable, step_variable):\n",
    "    c = DBSCAN(eps=variable, min_samples=15, metric='precomputed')\n",
    "    clusters = cl.cluster(distance_matrix, c)\n",
    "    dict_cluster = cl.tab_clusters_to_dict(clusters)\n",
    "    silhouette = silhouette_score(distance_matrix, clusters, metric=\"precomputed\")\n",
    "\n",
    "    nb_mini_clusters= 0\n",
    "    nb_big_clusters = 0\n",
    "    mean = 0\n",
    "    for i in range(len(dict_cluster)-1):\n",
    "        if(len(dict_cluster[i]) > 15):\n",
    "            nb_big_clusters += 1\n",
    "        elif(len(dict_cluster[i]) == 1):\n",
    "            nb_mini_clusters += 1\n",
    "        mean+=len(dict_cluster[i])\n",
    "        \n",
    "    tab_nb_clusters.append(len(dict_cluster)-1)\n",
    "    tab_noise.append(clusters.tolist().count(-1))\n",
    "    tab_nb_mini_clusters.append(nb_mini_clusters)\n",
    "    tab_nb_big_clusters.append(nb_big_clusters)\n",
    "    tab_mean.append(mean/(len(dict_cluster)-1))\n",
    "    tab_silhouette.append(silhouette)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):    \n",
    "    \n",
    "    fig = plt.figure(figsize=graph_size)\n",
    "    ax = plt.axes()\n",
    "    x = np.arange(start_variable, stop_variable, step_variable)\n",
    "    plt.plot(x, tab_nb_mini_clusters, color='red', linewidth=3.5, label='Mini clusters')\n",
    "    plt.plot(x, tab_nb_big_clusters, color='blue', linewidth=3.5, label='Big clusters')\n",
    "    \n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):\n",
    "    fig = plt.figure(figsize=graph_size)\n",
    "    ax = plt.axes()\n",
    "    x = np.arange(start_variable, stop_variable, step_variable)\n",
    "    plt.plot(x, tab_nb_clusters, color='red', linewidth=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):  \n",
    "    \n",
    "    fig = plt.figure(figsize=graph_size)\n",
    "    ax = plt.axes()\n",
    "    x = np.arange(start_variable, stop_variable, step_variable)\n",
    "    plt.plot(x, tab_silhouette, color='blue', linewidth=3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (project_folder == \"veleval\"):\n",
    "    config_clustering = {\"eps\": 0.4, \"min_samples\": 5, \"n_clusters_divider\": 10}\n",
    "else:\n",
    "    config_clustering = {\"eps\": 0.35, \"min_samples\": 10, \"n_clusters_divider\": 15}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 clusters\n",
      "noise: 81\n",
      "\n",
      "silhouette score : 0.38988872319145457\n",
      "\n",
      "mean size : 11.054054054054054\n",
      "\n",
      "16 big clusters: [0, 1, 4, 46, 18, 30, 12, 16, 19, 24, 45, 29, 33, 35, 36, 62]\n",
      "11 mini clusters : [2, 5, 6, 38, 48, 53, 55, 56, 60, 64, 71]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=750)\n",
    "#distance_matrix_reduced = pca.fit_transform(distance_matrix)\n",
    "\n",
    "dbscan = DBSCAN(eps=config_clustering[\"eps\"], min_samples=config_clustering[\"min_samples\"], metric='precomputed')\n",
    "optics = OPTICS(min_samples=10, metric='precomputed')\n",
    "\n",
    "clustering = dbscan\n",
    "X = distance_matrix\n",
    "\n",
    "tab_cluster = cl.cluster(X, clustering)\n",
    "dict_cluster = cl.tab_clusters_to_dict(tab_cluster)\n",
    "\n",
    "print(len(dict_cluster)-1, \"clusters\")\n",
    "print(\"noise:\", tab_cluster.tolist().count(-1))\n",
    "print()\n",
    "cl.cluster_properties(dict_cluster, distance_matrix, tab_cluster, \"precomputed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(save_dbscan_oservations):\n",
    "    print(\"Saving DBSCAN observations...\")\n",
    "    with open(\"files/\"+project_folder+\"/clustering/dbscan_observations.dict\",'wb') as outfile:\n",
    "        pickle.dump(dict_cluster, outfile)\n",
    "    with open(\"files/\"+project_folder+\"/clustering/dbscan_observations.tab\",'wb') as outfile:\n",
    "        pickle.dump(tab_cluster, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>route_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.7598</td>\n",
       "      <td>4.87564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.7595</td>\n",
       "      <td>4.87557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.7594</td>\n",
       "      <td>4.87589</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.7583</td>\n",
       "      <td>4.87658</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45.7561</td>\n",
       "      <td>4.87612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>45.7785</td>\n",
       "      <td>4.80798</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>45.7805</td>\n",
       "      <td>4.80921</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>45.7806</td>\n",
       "      <td>4.80916</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>45.7827</td>\n",
       "      <td>4.81093</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>45.7831</td>\n",
       "      <td>4.80931</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28508 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lat      lon route_num\n",
       "0   45.7598  4.87564         0\n",
       "1   45.7595  4.87557         0\n",
       "2   45.7594  4.87589         0\n",
       "3   45.7583  4.87658         0\n",
       "4   45.7561  4.87612         0\n",
       "..      ...      ...       ...\n",
       "61  45.7785  4.80798       898\n",
       "62  45.7805  4.80921       898\n",
       "63  45.7806  4.80916       898\n",
       "64  45.7827  4.81093       898\n",
       "65  45.7831  4.80931       898\n",
       "\n",
       "[28508 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):\n",
    "    num_cluster = 0\n",
    "    print(len(dict_cluster[num_cluster]))\n",
    "    #dp.display_routes(df_simplified, dict_cluster[nb_cluster])\n",
    "    map = dp.display_cluster_heatmap(df_simplified, dict_cluster[num_cluster])\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voxel Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clusters_used = []\\nfor key in dict_voxels_bikepath:\\n    if key in dict_voxels_pathfinding:\\n        if(dict_voxels_bikepath[key][\"cluster\"] not in clusters_used):\\n            dict_voxels_pathfinding[key][\"cluster\"] = len(clusters_used)\\n            clusters_used.append(dict_voxels_bikepath[key][\"cluster\"])\\n        else:\\n            dict_voxels_pathfinding[key][\"cluster\"] = clusters_used.index(dict_voxels_bikepath[key][\"cluster\"])'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''clusters_used = []\n",
    "for key in dict_voxels_bikepath:\n",
    "    if key in dict_voxels_pathfinding:\n",
    "        if(dict_voxels_bikepath[key][\"cluster\"] not in clusters_used):\n",
    "            dict_voxels_pathfinding[key][\"cluster\"] = len(clusters_used)\n",
    "            clusters_used.append(dict_voxels_bikepath[key][\"cluster\"])\n",
    "        else:\n",
    "            dict_voxels_pathfinding[key][\"cluster\"] = clusters_used.index(dict_voxels_bikepath[key][\"cluster\"])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11146\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for key in dict_voxels_pathfinding:\n",
    "    if(\"cluster\" not in dict_voxels_pathfinding[key]):\n",
    "        vox_str = key.split(\";\")\n",
    "        vox_int = [int(vox_str[0]), int(vox_str[1])]\n",
    "        tab_points = voxel.get_voxel_points(vox_int)\n",
    "        X.append([tab_points[0][0], tab_points[0][1], dict_voxels_pathfinding[key][\"cyclability_coeff\"]])\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silhouette score : 0.5486184250338909\n",
      "\n",
      "mean size : 10.014375561545373\n",
      "\n",
      "0 big clusters: []\n",
      "1067 mini clusters : [179, 384, 369, 562, 795, 1073, 1000, 460, 304, 753, 711, 310, 312, 726, 624, 701, 1019, 296, 162, 867, 636, 319, 568, 1003, 1050, 248, 24, 283, 642, 518, 594, 135, 492, 174, 409, 217, 893, 333, 46, 922, 322, 818, 601, 606, 1091, 1042, 366, 421, 439, 586, 498, 226, 1014, 831, 610, 760, 915, 29, 17, 480, 761, 887, 556, 356, 1044, 1006, 212, 22, 520, 781, 390, 891, 255, 754, 72, 263, 814, 516, 844, 468, 374, 902, 363, 257, 19, 980, 405, 1045, 791, 150, 457, 912, 300, 874, 669, 694, 825, 679, 880, 991, 644, 659, 111, 266, 1095, 160, 18, 313, 652, 749, 963, 1081, 801, 971, 1004, 979, 397, 204, 705, 728, 581, 219, 692, 416, 177, 670, 389, 71, 1062, 823, 305, 1015, 408, 532, 1108, 414, 233, 1102, 288, 69, 579, 464, 1012, 171, 950, 762, 144, 674, 32, 708, 145, 800, 251, 535, 1022, 92, 836, 315, 619, 994, 354, 719, 94, 513, 380, 117, 895, 1068, 706, 697, 434, 188, 224, 1020, 914, 756, 395, 294, 542, 383, 1047, 667, 646, 661, 484, 721, 1079, 482, 426, 683, 491, 928, 289, 1028, 846, 565, 742, 806, 607, 978, 987, 531, 187, 695, 308, 452, 766, 920, 450, 272, 489, 802, 59, 827, 517, 302, 665, 371, 281, 939, 654, 512, 84, 981, 730, 364, 857, 629, 133, 916, 1066, 684, 655, 1094, 1031, 435, 157, 465, 136, 433, 87, 41, 906, 990, 985, 1002, 847, 486, 903, 566, 51, 508, 767, 213, 747, 999, 463, 685, 270, 66, 797, 686, 474, 983, 39, 793, 505, 320, 200, 663, 834, 413, 4, 399, 722, 855, 1041, 377, 100, 561, 1026, 632, 995, 129, 553, 765, 336, 9, 1018, 948, 1025, 997, 941, 63, 351, 729, 543, 803, 74, 558, 456, 882, 1060, 1039, 166, 630, 633, 848, 2, 246, 651, 842, 716, 265, 687, 442, 613, 203, 822, 1, 417, 244, 1056, 622, 485, 1007, 600, 198, 621, 146, 900, 1069, 375, 970, 588, 323, 1034, 1055, 530, 56, 361, 109, 772, 218, 863, 124, 548, 396, 30, 892, 264, 617, 36, 216, 851, 826, 541, 883, 291, 815, 343, 680, 672, 953, 156, 551, 771, 1109, 744, 97, 1092, 239, 151, 93, 503, 153, 1058, 897, 75, 301, 1107, 840, 334, 759, 1049, 342, 962, 737, 936, 810, 638, 446, 88, 605, 974, 945, 38, 527, 934, 1071, 693, 1090, 499, 982, 717, 222, 447, 671, 745, 330, 40, 774, 700, 839, 401, 339, 576, 16, 326, 91, 764, 938, 115, 635, 441, 378, 121, 1086, 454, 202, 933, 649, 422, 96, 237, 790, 191, 47, 261, 637, 119, 718, 989, 869, 249, 852, 314, 837, 284, 448, 763, 1104, 461, 196, 103, 937, 678, 112, 181, 870, 789, 850, 1093, 35, 668, 657, 612, 714, 180, 1037, 170, 639, 890, 1023, 1080, 247, 798, 316, 523, 340, 830, 864, 984, 497, 155, 549, 813, 1100, 352, 473, 118, 666, 105, 1048, 964, 348, 519, 1082, 1064, 580, 387, 904, 487, 3, 582, 808, 1061, 45, 599, 603, 720, 449, 488, 710, 909, 888, 596, 381, 225, 1098, 879, 350, 479, 8, 359, 260, 676, 658, 34, 197, 901, 472, 739, 602, 459, 1105, 243, 268, 21, 608, 1005, 755, 194, 740, 643, 89, 809, 704, 42, 229, 99, 907, 587, 969, 141, 195, 143, 563, 477, 1063, 250, 824, 419, 25, 724, 481, 367, 120, 427, 1112, 338, 353, 1084, 664, 1077, 79, 407, 862, 1043, 70, 917, 921, 509, 126, 394, 253, 821, 856, 751, 597, 894, 373, 183, 578, 946, 376, 169, 536, 31, 930, 462, 57, 201, 504, 286, 896, 60, 293, 1087, 102, 696, 205, 529, 256, 332, 843, 6, 817, 476, 942, 337, 641, 430, 812, 298, 1027, 773, 451, 161, 1017, 400, 569, 571, 958, 794, 185, 698, 609, 85, 466, 707, 699, 199, 832, 625, 125, 778, 379, 966, 307, 820, 175, 816, 62, 1067, 709, 972, 424, 123, 360, 884, 131, 50, 956, 128, 362, 1106, 575, 572, 776, 511, 368, 49, 349, 206, 67, 967, 746, 23, 554, 611, 873, 623, 923, 309, 769, 992, 327, 881, 190, 1013, 777, 725, 1054, 593, 988, 805, 54, 1088, 1029, 142, 443, 662, 957, 924, 918, 712, 889, 311, 1053, 148, 392, 592, 77, 393, 1089, 107, 955, 557, 139, 845, 267, 52, 134, 787, 252, 965, 1072, 365, 977, 110, 819, 410, 290, 27, 1111, 628, 438, 618, 412, 838, 386, 996, 83, 615, 1010, 420, 231, 137, 355, 444, 510, 228, 647, 432, 871, 859, 1110, 528, 328, 926, 533, 627, 90, 127, 925, 908, 11, 1097, 469, 1024, 677, 1099, 277, 853, 189, 796, 53, 506, 783, 943, 467, 234, 653, 782, 178, 784, 792, 1016, 163, 952, 276, 385, 715, 98, 727, 758, 168, 515, 786, 841, 616, 741, 490, 733, 583, 614, 675, 1075, 811, 306, 932, 37, 875, 951, 455, 886, 750, 1030, 574, 1059, 702, 101, 1036, 961, 522, 80, 1101, 325, 524, 358, 570, 626, 998, 604, 335, 1035, 935, 210, 245, 1011, 232, 1009, 833, 14, 341, 968, 436, 1046, 785, 731, 645, 318, 538, 927, 494, 648, 10, 214, 940, 757, 931, 347, 1065, 58, 207, 595, 944, 590, 475, 429, 282, 713, 279, 660, 1038, 905, 238, 372, 752, 173, 48, 736, 223, 61, 919, 690, 43, 525, 1113, 258, 567, 681, 1083, 1033, 78, 545, 770, 130, 780, 370, 1001, 514, 743, 209, 140, 73, 1078, 324, 297, 15, 44, 620, 172, 865, 114, 158, 913, 547, 1051, 1040, 345, 910, 1008, 147, 55, 423, 828, 295, 278, 650, 259, 640, 235, 428, 560, 406, 500, 986, 106, 269, 866, 33, 703, 689, 858, 285, 949, 357, 402, 868, 95, 471, 1052, 193, 804, 331, 149, 732, 108, 947, 723, 878, 437, 415, 1032, 221, 440, 929, 877, 577, 280, 899, 1070, 540, 835, 82, 241, 598, 388, 329, 688, 738, 7, 1076, 591, 254, 559, 317, 748, 564, 470, 872, 734, 775, 993, 132, 425, 483, 122, 673, 458, 954, 876, 631, 898, 184, 973, 299, 779, 164, 478, 68, 344, 26, 656, 431, 152, 445, 768, 159, 208, 1103, 634, 236, 167, 539, 76, 1057, 537, 240, 861, 12, 829, 526, 502, 215, 453, 1074, 544, 113, 1021, 0, 799, 271, 521, 116, 976, 589, 186, 507, 104, 860, 735, 849, 546, 493, 911, 303, 220, 138, 555, 28, 5, 1085, 682, 1096]\n",
      "1113 clusters\n",
      "noise: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if(new_voxels_clustering):  \n",
    "    kmeans = KMeans(n_clusters=len(X)//config_clustering[\"n_clusters_divider\"], random_state=0)\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    clusters = cl.cluster(X, kmeans)\n",
    "\n",
    "else:\n",
    "    with open(\"files/\"+project_folder+\"/clustering/kmeans_voxels_osmnx.sk\",'rb') as infile:\n",
    "        kmeans = pickle.load(infile)\n",
    "    clusters = kmeans.labels_\n",
    "        \n",
    "dict_cluster_voxel = cl.tab_clusters_to_dict(clusters)\n",
    "\n",
    "cl.cluster_properties(dict_cluster_voxel, X, clusters)\n",
    "print(len(dict_cluster_voxel)-1, \"clusters\")\n",
    "print(\"noise:\", clusters.tolist().count(-1))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kmeans_voxels_osmnx.sk...\n"
     ]
    }
   ],
   "source": [
    "if(save_voxels_clustering):\n",
    "    print(\"Saving kmeans_voxels_osmnx.sk...\")\n",
    "    with open(\"files/\"+project_folder+\"/clustering/kmeans_voxels_osmnx.sk\",'wb') as outfile:\n",
    "        pickle.dump(kmeans, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(display):\n",
    "\n",
    "    num_cluster = 792\n",
    "\n",
    "    tab = []\n",
    "    for vox in dict_cluster_voxel[num_cluster]:\n",
    "        tab.append(X[vox])\n",
    "\n",
    "    df = pd.DataFrame(tab, columns=[\"lat\", \"lon\", \"value\"])\n",
    "    '''fig = px.scatter_mapbox(df, lat=\"lat\", lon=\"lon\",  color=\"value\", size=\"value\", zoom=10)\n",
    "    fig.show()'''\n",
    "    \n",
    "    map = folium.Map(location=[df.iloc[0][\"lat\"],df.iloc[0][\"lon\"]], control_scale=True, zoom_start=11)\n",
    "    HeatMap(data=df.values.tolist(), radius=8, max_zoom=13).add_to(map)\n",
    "    \n",
    "map\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving voxels_clustered_osmnx.dict...\n"
     ]
    }
   ],
   "source": [
    "dict_voxels_pathfinding_clustered = deepcopy(dict_voxels_pathfinding)\n",
    "for c in dict_cluster_voxel:\n",
    "    for vox in dict_cluster_voxel[c]:\n",
    "        v = X[vox]\n",
    "        vox_int = voxel.find_voxel_int([v[0], v[1]], False)\n",
    "        key = str(vox_int[0])+\";\"+str(vox_int[1])\n",
    "        dict_voxels_pathfinding_clustered[key][\"cluster\"] = c #len(clusters_used)+c\n",
    "if(save_voxels_clustering):\n",
    "    print(\"Saving voxels_clustered_osmnx.dict...\")\n",
    "    with open(\"files/\"+project_folder+\"/clustering/voxels_clustered_osmnx.dict\",'wb') as outfile:\n",
    "        pickle.dump(dict_voxels_pathfinding_clustered, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11146\n"
     ]
    }
   ],
   "source": [
    "print(len(dict_voxels_pathfinding_clustered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Osmnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898 1\n"
     ]
    }
   ],
   "source": [
    "with open(\"files/\"+project_folder+\"/clustering/dbscan_observations.tab\",'rb') as infile:\n",
    "    tab_clusters = pickle.load(infile)\n",
    "print(df_pathfinding.iloc[-1][\"route_num\"], tab_clusters[df_pathfinding.iloc[-1][\"route_num\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_lyon = ox.graph_from_point((45.74846, 4.84671), dist=7500)\n",
    "#G_stetienne = ox.graph_from_point((45.4333, 4.4), dist=7500)\n",
    "#G_montreal = ox.graph_from_place('montreal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/\"+project_folder+\"/city_graphs/city.ox\", \"rb\") as infile:\n",
    "    G = pickle.load(infile)\n",
    "\n",
    "nodes, _ = ox.graph_to_gdfs(G)\n",
    "tree = KDTree(nodes[['y', 'x']], metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing neural network...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'bidirectional' and 'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-52321524421b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mvoxels_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNN_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtab_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"files/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mproject_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/neural_networks/network_osmnx.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'bidirectional' and 'dropout'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if(not(os.path.isfile(\"files/\"+project_folder+\"/neural_networks/network_osmnx.pt\"))):\n",
    "    print(\"No neural network found.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Testing neural network...\")\n",
    "    import python.validation as validation\n",
    "\n",
    "    with open(\"files/\"+project_folder+\"/clustering/dbscan_observations.tab\",'rb') as infile:\n",
    "        tab_clusters = pickle.load(infile)\n",
    "    with open(\"files/\"+project_folder+\"/clustering/voxels_clustered_osmnx.dict\",'rb') as infile:\n",
    "        dict_voxels = pickle.load(infile)\n",
    "    with open(\"files/\"+project_folder+\"/clustering/kmeans_voxels_osmnx.sk\",'rb') as infile:\n",
    "        kmeans = pickle.load(infile)\n",
    "\n",
    "    df = df_pathfinding\n",
    "\n",
    "    size_data = 1\n",
    "    hidden_size = 256\n",
    "    num_layers = 2\n",
    "    voxels_frequency = 4\n",
    "\n",
    "    network = RNN.RNN_LSTM(size_data, max(tab_clusters)+1, hidden_size, num_layers)\n",
    "    network.load_state_dict(torch.load(\"files/\"+project_folder+\"/neural_networks/network_osmnx.pt\"))\n",
    "    network.eval()\n",
    "\n",
    "    nb_good_predict = 0\n",
    "    nb_predict = 0\n",
    "\n",
    "    deviation = 0 #5e-3\n",
    "\n",
    "    tab_predict = []\n",
    "\n",
    "    for i in range(10): #len(tab_clusters)):\n",
    "        if(tab_clusters[i] != -1):\n",
    "            #print(i)\n",
    "            df_temp = df[df[\"route_num\"]==i]\n",
    "            d_point = [df_temp.iloc[0][\"lat\"], df_temp.iloc[0][\"lon\"]]\n",
    "            f_point = [df_temp.iloc[-1][\"lat\"], df_temp.iloc[-1][\"lon\"]]\n",
    "            rand = random.uniform(-deviation, deviation)\n",
    "            d_point[0] += rand\n",
    "            rand = random.uniform(-deviation, deviation)\n",
    "            d_point[1] += rand\n",
    "            rand = random.uniform(-deviation, deviation)\n",
    "            f_point[0] += rand\n",
    "            rand = random.uniform(-deviation, deviation)\n",
    "            f_point[1] += rand\n",
    "\n",
    "            if(d_point[0] > 45.5 or project_folder != \"veleval\"):\n",
    "                df_route, cl, nb_new_cluster = validation.find_cluster(d_point, f_point, network, voxels_frequency, df_pathfinding, dict_voxels, \n",
    "                                             kmeans, tree, G, False)\n",
    "                if(cl == tab_clusters[i]):\n",
    "                    nb_good_predict += 1\n",
    "                    #print(\"good predict\")\n",
    "                nb_predict += 1\n",
    "    if(nb_predict > 0):\n",
    "        tab_predict.append(nb_good_predict/nb_predict)\n",
    "\n",
    "    tot_predict = 0\n",
    "    for predict in tab_predict:\n",
    "        tot_predict += predict\n",
    "    print(\"ratio:\", tot_predict/len(tab_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.display_mapbox(df_pathfinding[df_pathfinding[\"route_num\"]==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_stetienne\n",
    "G = G_lyon\n",
    "with open(\"files/\"+project_folder+\"/bikepath_fusioned.df\", \"rb\") as outfile:\n",
    "    df_osm_bikepath = pickle.load(outfile)\n",
    "    df_osm_bikepath = pd.DataFrame(df_osm_bikepath, dtype=object)\n",
    "      \n",
    "_, dict_voxels_osm_bikepath = voxel.generate_voxels(df_osm_bikepath, df_osm_bikepath.iloc[0][\"route_num\"], df_osm_bikepath.iloc[-1][\"route_num\"], True)\n",
    "\n",
    "for v in G:\n",
    "    for v_n in G[v]:\n",
    "        df_line = pd.DataFrame([[G.nodes[v]['y'], G.nodes[v]['x'], 1], [G.nodes[v_n]['y'], G.nodes[v_n]['x'], 1]], columns=[\"lat\", \"lon\", \"route_num\"])\n",
    "        tab_voxels, _ = voxel.generate_voxels(df_line, 1, 1)\n",
    "        nb_vox_found = 0\n",
    "        tot_coeff = 0\n",
    "        for vox in tab_voxels[0]:\n",
    "            if vox in dict_voxels_osm_bikepath:\n",
    "                G[v][v_n][0]['length'] -= G[v][v_n][0]['length']*0.15\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/\"+project_folder+\"/data_processed/dbscan_pathfinding_osmnx.dict\",'rb') as infile:\n",
    "    dict_cluster = pickle.load(infile)\n",
    "    \n",
    "new_dict = {}\n",
    "    \n",
    "for key in dict_cluster:\n",
    "    print(dict_cluster[key])\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
